{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essense test by Marco Wong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this jupyter notebook is divided into 4 parts:\n",
    "1. Set up dummy data and utils functions\n",
    "2. SQL case study\n",
    "3. SQL questions\n",
    "4. Python questions\n",
    "    \n",
    "In order to run the set up scripts, you might need to create a new environment in your local:\n",
    "- conda create -n essense_test python=3.7\n",
    "- conda activate essense_test\n",
    "- pip install SQLAlchemy==1.4.23\n",
    "- pip install pandas==1.3.2\n",
    "- pip install psycopg2==2.7.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Setting up Postgres DB with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 Setting up Postgres DB with dummy data  \n",
    "I have created a Postgres DB with some dummy data for the case study and SQL question in part 2.  \n",
    "I would like to set it up so we can go through the queries with some data.  \n",
    "\n",
    "Normally I wouldn't store any database credentials in the code, but since this will only be shared within the interviewer and myself, so I think it is not neccessary to create a yaml file and overcomplicate the task.  \n",
    "\n",
    "Please ignore this part if you want to go straight into case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def create_connection():\n",
    "    host = 'tai.db.elephantsql.com'\n",
    "    user = 'xbsstbyy'\n",
    "    db = 'xbsstbyy'\n",
    "    pwd = 'mpPWwkb3YjnyKUjYGgunh5C_3PZyjuls'\n",
    "    engine = s.create_engine(f'postgresql://{user}:{pwd}@{host}/{db}')\n",
    "    return engine\n",
    "\n",
    "def query(q):\n",
    "    conn = create_connection()\n",
    "    if 'update' not in q and 'create' not in q and 'insert' not in q and 'select' in q:\n",
    "        return pd.read_sql_query(q, con=conn)\n",
    "    else:\n",
    "        conn.execute(q)\n",
    "        return True\n",
    "\n",
    "def db_insert(data, target):\n",
    "    conn = create_connection()\n",
    "    insert_statement = ''\n",
    "    cols = ','.join(data.columns)\n",
    "    for i in range(0, data.shape[0]):\n",
    "        values = tuple(list(data.iloc[i, :]))\n",
    "        values = str(values) if len(values) > 1 else str(values).replace(',', '')\n",
    "        values = values.replace(\"'null'\", \"null\").replace('\"', \"'\")\n",
    "        insert_statement += f'insert into public.{target} ({cols}) values {values};'\n",
    "    conn.execute(insert_statement)\n",
    "    conn.dispose()\n",
    "    return True\n",
    "\n",
    "#set up tables and data\n",
    "table_q = '''\n",
    "drop table if exists public.video_best_practices_data;\n",
    "drop table if exists public.banner_best_practices_data;\n",
    "drop table if exists public.creative_testing_tracker_data;\n",
    "drop table if exists public.results_data;\n",
    "drop table if exists public.Table_1;\n",
    "\n",
    "create table public.video_best_practices_data\n",
    "(\n",
    "campaign_name varchar(100),\n",
    "media_plan_id int,\n",
    "creative_name varchar(100),\n",
    "video_bp_count int\n",
    ");\n",
    "\n",
    "create table public.banner_best_practices_data\n",
    "(\n",
    "campaign_name varchar(100),\n",
    "media_plan_id int,\n",
    "creative_name varchar(100),\n",
    "banner_bp_count int\n",
    ");\n",
    "\n",
    "create table public.creative_testing_tracker_data\n",
    "(\n",
    "media_plan_id int,\n",
    "media_plan_name varchar(100),\n",
    "creative_name varchar(100),\n",
    "pri_passed char(3)\n",
    ");\n",
    "\n",
    "create table public.results_data\n",
    "(\n",
    "campaign_name varchar(100),\n",
    "media_plan_id int,\n",
    "product varchar(100),\n",
    "reach int,\n",
    "abs_lift float,\n",
    "spends int\n",
    ");\n",
    "\n",
    "create table public.Table_1 \n",
    "(\n",
    "campaign_id int,\n",
    "Channel varchar(50),\n",
    "Exposed_Count int,\n",
    "Control_Count int,\n",
    "Exposed_Percent float,\n",
    "Control_Percent float,\n",
    "Sig_Reported char(1),\n",
    "Lift_Reported float\n",
    ");\n",
    "'''\n",
    "\n",
    "query(table_q)\n",
    "        \n",
    "campaign_name = ['essense_campaign_1', 'essense_campaign_2', 'essense_campaign_3']\n",
    "media_plan_id = [1, 2, 3]\n",
    "media_plan_name = ['m1', 'm2', 'm2']\n",
    "product = ['google', 'youtube', 'bing']\n",
    "creative_name = ['creative_1', 'creative_2', 'creative_3']\n",
    "\n",
    "campaign_name = campaign_name\n",
    "media_plan_id = media_plan_id\n",
    "media_plan_name = media_plan_name\n",
    "product = product\n",
    "creative_name = creative_name\n",
    "\n",
    "random.shuffle(campaign_name)\n",
    "random.shuffle(media_plan_id)\n",
    "random.shuffle(product)\n",
    "random.shuffle(creative_name)\n",
    "\n",
    "result_data = {'campaign_name': campaign_name,\n",
    "               'media_plan_id': media_plan_id,\n",
    "               'product': product,\n",
    "               'reach': [random.randint(0,22) for i in campaign_name],\n",
    "               'abs_lift': np.random.uniform(low=0.0, high=1.0, size=len(campaign_name)),\n",
    "               'spends': [random.randint(0,22) for i in campaign_name]\n",
    "               }\n",
    "creative_testing_tracker_data = {'media_plan_id': media_plan_id,\n",
    "                                 'media_plan_name': media_plan_name,\n",
    "                                 'creative_name': creative_name,\n",
    "                                 'pri_passed': ['YES' if random.randint(0,22) > 10 else 'NO' for i in media_plan_id]\n",
    "                                }\n",
    "banner_best_practices_data = {'campaign_name': campaign_name,\n",
    "                              'media_plan_id': media_plan_id,\n",
    "                              'creative_name': creative_name,\n",
    "                              'banner_bp_count': [random.randint(0,22) for i in media_plan_id]}\n",
    "video_best_practices_data = {'campaign_name': campaign_name,\n",
    "                             'media_plan_id': media_plan_id,\n",
    "                             'creative_name': creative_name,\n",
    "                             'video_bp_count': [random.randint(0,22) for i in media_plan_id]}\n",
    "table_1 = {'campaign_id': [1, 2, 3, 4],\n",
    "           'Channel': ['YouTube', 'Facebook', 'Twitter', 'YouTube'],\n",
    "           'Exposed_Count': [1000, 800, 700, 2000],\n",
    "           'Control_Count': [1500, 820, 750, 2000],\n",
    "           'Exposed_Percent': [0.78, 0.45, 0.51, 0.63],\n",
    "           'Control_Percent': [0.76, 0.42, 0.51, 0.629],\n",
    "           'Sig_Reported': ['Y', 'Y', 'N', 'N'],\n",
    "           'Lift_Reported': [0.02, 0.03, 0, 0.001]\n",
    "           }\n",
    "\n",
    "results_data = pd.DataFrame(result_data)\n",
    "creative_testing_tracker_data = pd.DataFrame(creative_testing_tracker_data)\n",
    "banner_best_practices_data = pd.DataFrame(banner_best_practices_data)\n",
    "video_best_practices_data = pd.DataFrame(video_best_practices_data)\n",
    "table_1 = pd.DataFrame(table_1)\n",
    "\n",
    "db_insert(results_data, 'results_data')\n",
    "db_insert(creative_testing_tracker_data, 'creative_testing_tracker_data')\n",
    "db_insert(banner_best_practices_data, 'banner_best_practices_data')\n",
    "db_insert(video_best_practices_data, 'video_best_practices_data')\n",
    "db_insert(table_1, 'Table_1')\n",
    "\n",
    "# add variation\n",
    "query(''' insert into creative_testing_tracker_data\n",
    "values\n",
    "(1, 'm1', 'creative_4', 'YES'),\n",
    "(1, 'm1', 'creative_5', 'YES'),\n",
    "(1, 'm1', 'creative_6', 'NO'),\n",
    "(2, 'm2', 'creative_7', 'NO'),\n",
    "(2, 'm2', 'creative_8', 'YES'),\n",
    "(2, 'm2', 'creative_9', 'NO');\n",
    "\n",
    "insert into banner_best_practices_data\n",
    "values\n",
    "('essense_campaign_1', 1, 'creative_4', 5),\n",
    "('essense_campaign_1', 1, 'creative_5', 1),\n",
    "('essense_campaign_1', 1, 'creative_6', 2),\n",
    "('essense_campaign_2', 2, 'creative_8', 10),\n",
    "('essense_campaign_2', 2, 'creative_9', 8);\n",
    "\n",
    "insert into video_best_practices_data\n",
    "values\n",
    "('essense_campaign_1', 1, 'creative_4', 10),\n",
    "('essense_campaign_1', 1, 'creative_6', 20),\n",
    "('essense_campaign_2', 2, 'creative_9', 9)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 SQL Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship between tables:\n",
    "After reading the case study, I think the relationship of the tables should be similar to the following image.  \n",
    "  \n",
    "![](relationship.jpeg)\n",
    "  \n",
    "Notes:\n",
    "1. campaign_name is one-to-many to media_plan_id\n",
    "2. media_plan_id is one-to-many to creative_name\n",
    "3. media_plan_id is one-to-one to media_plan_name\n",
    "4. Not all campaign/media_plan have data in video/banner best practice (need to accomodate with nulls)\n",
    "5. product is specific to media_plan_id\n",
    "6. creative_name in creative_testing_trackerdata is fk to creative_name in creative_name in video/banner best practice\n",
    "\n",
    "Assumptions:  \n",
    "1. There is no duplicated campaign_name, media_plan_id and creative_name combination on video_best_practices_data and banner_best_practices_data tables (unique constraint or pk)  \n",
    "2. There is no duplicated campaign_name, media_plan_id, product combination on results_data  \n",
    "3. There is no duplicated media_plan_id, media_name, creative_name combination in creative_testing_tracker_data  media_plan_id column is int instead of str in results_data  \n",
    "4. Assume all combinations of media_plan_id and creative_name in video/banner best practices data exists in creative_testing_tracker_data\n",
    "\n",
    "Approach:\n",
    "I created 4 common table expressions to make the view to be easier to follow.\n",
    "1. Perform full outer join video/banner_best_practices_data on campaign_name, media_plan_id and creative_name with case when null to accomodate note 4. Then we will get a table with count of banner/video_best_practices for each capaign_name, media_plan_id and creative_name combination (video_banner_merge)  \n",
    "  \n",
    "2. video_banner_merge join to creative_testing_tracker_data on media_plan_id and creative_name, then we would know which media_plan_id and creative_name combination has achieved primary goal (video_banner_practices_cte)  \n",
    "  \n",
    "3. Aggregate video_banner_practices_cte by campaign_name and media_plan_id to get number of creatives, number of best creatives (banner_bp_count >= 8 and video_bp_count >= 9), number of creatives achieved primary goal. (agg_vb_practices)  \n",
    "  \n",
    "4. Calculate cost per lifted customer using results_data (cpil)\n",
    "\n",
    "5. agg_vb_practices left join to cpil on campaign_name and media_plan_id to get the final result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        campaign_name  media_plan_id  product      cpil  \\\n",
      "0  essense_campaign_2              3     bing  8.366990   \n",
      "1  essense_campaign_3              2  youtube  3.706291   \n",
      "2  essense_campaign_1              1   google  6.162566   \n",
      "\n",
      "   creatives_passing_best_practice_percentage  \\\n",
      "0                                        0.00   \n",
      "1                                        0.00   \n",
      "2                                        0.25   \n",
      "\n",
      "   creatives_achieving_goal_percentage  \n",
      "0                                 1.00  \n",
      "1                                 1.00  \n",
      "2                                 0.75  \n"
     ]
    }
   ],
   "source": [
    "vb_q = ''' \n",
    "drop view if exists result_view;\n",
    "create view result_view as\n",
    "\n",
    "with video_banner_merge as (\n",
    "select \n",
    "case when vd.campaign_name is null then bd.campaign_name else vd.campaign_name end as campaign_name,\n",
    "case when vd.media_plan_id is null then bd.media_plan_id else vd.media_plan_id end as media_plan_id,\n",
    "case when vd.creative_name is null then bd.creative_name else vd.creative_name end as creative_name,\n",
    "case when bd.banner_bp_count is null then 0 else bd.banner_bp_count end as banner_bp_count,\n",
    "case when vd.video_bp_count is null then 0 else vd.video_bp_count end as video_bp_count\n",
    "from video_best_practices_data vd\n",
    "full outer join banner_best_practices_data bd\n",
    "on vd.campaign_name = bd.campaign_name\n",
    "and vd.media_plan_id = bd.media_plan_id \n",
    "and vd.creative_name = bd.creative_name\n",
    "),\n",
    "\n",
    "video_banner_practices_cte as (\n",
    "select \n",
    "vb.campaign_name,\n",
    "c.media_plan_id,\n",
    "c.creative_name,\n",
    "c.pri_passed,\n",
    "case when vb.banner_bp_count is null then 0 else vb.banner_bp_count end banner_bp_count,\n",
    "case when vb.video_bp_count is null then 0 else vb.video_bp_count end video_bp_count\n",
    "from\n",
    "creative_testing_tracker_data c\n",
    "left join video_banner_merge vb\n",
    "on c.media_plan_id = vb.media_plan_id and c.creative_name = vb.creative_name),\n",
    "\n",
    "agg_vb_practices as\n",
    "(\n",
    "select\n",
    "campaign_name,\n",
    "media_plan_id,\n",
    "count(creative_name) creatives,\n",
    "sum(banner_bp_count) banner_bp_count,\n",
    "sum(video_bp_count) video_bp_count,\n",
    "sum(case when banner_bp_count >= 8 and video_bp_count >= 9 then 1 else 0 end) best_practice_creatives,\n",
    "sum(case when pri_passed = 'YES' then 1 else 0 end) successful_creatives,\n",
    "sum(case when pri_passed = 'NO' then 1 else 0 end) fail_creatives\n",
    "from video_banner_practices_cte\n",
    "group by campaign_name, media_plan_id),\n",
    "\n",
    "cpil as (\n",
    "select \n",
    "campaign_name,\n",
    "media_plan_id,\n",
    "product,\n",
    "case when reach*abs_lift = 0 then spends else spends/(reach*abs_lift) end cost_per_lifted\n",
    "from results_data)\n",
    "\n",
    "\n",
    "select \n",
    "c.campaign_name,\n",
    "c.media_plan_id,\n",
    "c.product,\n",
    "c.cost_per_lifted as cpil,\n",
    "cast(m.best_practice_creatives as float)/cast(m.creatives as float) as creatives_passing_best_practice_percentage,\n",
    "cast(m.successful_creatives as float)/cast(creatives as float) as creatives_achieving_goal_percentage\n",
    "from cpil c\n",
    "left join agg_vb_practices m\n",
    "on c.media_plan_id = m.media_plan_id and c.campaign_name = m.campaign_name\n",
    "'''\n",
    "\n",
    "query(vb_q)\n",
    "\n",
    "vb = query(''' select campaign_name,\n",
    "media_plan_id,\n",
    "product,\n",
    "cpil,\n",
    "creatives_passing_best_practice_percentage,\n",
    "creatives_achieving_goal_percentage\n",
    "from result_view ''')\n",
    "print(vb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 SQL Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we are converting the results based on one tailed tests to two tailed tests to allow the youtube data to be consistent with other channels.\n",
    "\n",
    "We are going to create a new column called \"Significance\" to replace the original \"Sig_Reported\" column, where it has the following logic:\n",
    "Significance = 'Y' if \"Lifted_Reported\" > MDE, 'N' otherwise\n",
    "MDE = 1.645*((Control_Percent*(1-Control_Percent)/Control_Count) + (Control_Percent*(1-Control_Percent)/Exposed_Count))^0.5\n",
    "\n",
    "The query below will insert the existing data from Table_1 into new_table_1, by removing Sig_Reported column and adding Significance column with 2-tailed test for Channel = 'YouTube'.\n",
    "\n",
    "Assumptions:\n",
    "- Channel = \"YouTube\" is case sensitive\n",
    "- Assume all other channels have the same significance level as \"YouTube\"\n",
    "- Assume all other channels have calculated Sig_Reported correctly and with the same significance level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   campaign_id   channel  exposed_count  control_count  exposed_percent  \\\n",
      "0            2  Facebook            800            820             0.45   \n",
      "1            3   Twitter            700            750             0.51   \n",
      "2            1   YouTube           1000           1500             0.78   \n",
      "3            4   YouTube           2000           2000             0.63   \n",
      "\n",
      "   control_percent significance  lift_reported  \n",
      "0            0.420            Y          0.030  \n",
      "1            0.510            N          0.000  \n",
      "2            0.760            N          0.020  \n",
      "3            0.629            N          0.001  \n"
     ]
    }
   ],
   "source": [
    "table_1_q = ''' \n",
    "drop table if exists new_table_1;\n",
    "\n",
    "select \n",
    "campaign_id, \n",
    "Channel, \n",
    "Exposed_Count, \n",
    "Control_Count, \n",
    "Exposed_Percent, \n",
    "Control_Percent, \n",
    "Sig_Reported Significance,\n",
    "Lift_Reported\n",
    "into new_table_1\n",
    "from Table_1\n",
    "where Channel != 'YouTube';\n",
    "\n",
    "with calculate_mde as (\n",
    "select\n",
    "campaign_id,\n",
    "Channel,\n",
    "Exposed_Count,\n",
    "Control_Count,\n",
    "Exposed_Percent,\n",
    "Control_Percent,\n",
    "Sig_Reported,\n",
    "Lift_Reported,\n",
    "case when Control_Count > 0 then (Control_Percent*(1-Control_Percent)/Control_Count) else 0 end Control_MDE,\n",
    "case when Exposed_Count > 0 then (Control_Percent*(1-Control_Percent)/Exposed_Count) else 0 end Expose_MDE\n",
    "from Table_1\n",
    "where Channel = 'YouTube'\n",
    ")\n",
    "\n",
    "insert into new_table_1\n",
    "(campaign_id, Channel, Exposed_Count, Control_Count, \n",
    "Exposed_Percent, Control_Percent, Significance, Lift_Reported)\n",
    "select \n",
    "campaign_id,\n",
    "Channel,\n",
    "Exposed_Count,\n",
    "Control_Count,\n",
    "Exposed_Percent,\n",
    "Control_Percent,\n",
    "case when Lift_Reported > 1.645*(Control_MDE+Expose_MDE)^0.5 then 'Y' else 'N' end Significance,\n",
    "Lift_Reported\n",
    "from calculate_mde\n",
    "'''\n",
    "query(table_1_q)\n",
    "\n",
    "table_1 = query(''' select\n",
    "campaign_id,\n",
    "Channel,\n",
    "Exposed_Count,\n",
    "Control_Count,\n",
    "Exposed_Percent,\n",
    "Control_Percent,\n",
    "Significance,\n",
    "Lift_Reported\n",
    "from new_table_1 ''')\n",
    "\n",
    "print(table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Comment:  \n",
    "Since this exercise says only YouTube has one-tailed test, so my approach is assuming all other channels have calculated the tests correctly. However, when I tried to calculate using the same formula given, Facebook gave me Significance = \"N\". Such that MDE is approximately 0.4 and Lifted_Reported = 0.03, so it will be \"N\".  \n",
    "Therefore I would double check the Sig_Reported is consistent between different channel again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04034689627876311\n"
     ]
    }
   ],
   "source": [
    "# checking facebook mde\n",
    "fb_con_count = 820\n",
    "fb_exp_count = 800\n",
    "fb_con_per = 0.42*(1-0.42)\n",
    "mde = ((fb_con_per/fb_con_count) + (fb_con_per/fb_exp_count))**0.5\n",
    "mde = 1.645*mde\n",
    "\n",
    "fb_lifted_reported = 0.03\n",
    "print(mde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Answer for fibonacci sequence with x = 10: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "\n",
      "2) Answer for divisible list with input my_list = [3, 4, 5, 6, 7, 8, 9] and y = 3: [3, 6, 9]\n",
      "\n",
      "3a) Answer for LCM using loop with inputs num1 = 9 and num2 = 63: 63\n",
      "3b) Answer for LCM using math package with inputs num1 = 9 and num2 = 63: 63\n",
      "\n",
      "4) Answer for mean, std and variance using numpy with inputs my_list = [1, 5, 3, 1, 2, 4, 6, 7, 1]: {'mean': 3.3333333333333335, 'std': 2.160246899469287, 'variance': 4.666666666666667}\n",
      "\n",
      "5) Answer for bucketing data\n",
      "    spends Group\n",
      "0   100000     3\n",
      "1   500000     3\n",
      "2  1000000     2\n",
      "3  2000000     1\n",
      "\n",
      "6) Answer for formating date from 08/20/2021 to 20212008\n",
      "\n",
      "7a) Answer for tidy up columns names from ['Ema@il', 'E$$ssense ', 'Pyt[]hon', 'cost_per_head  '] to ['Email', 'Essense', 'Python', 'costperhead']\n",
      "\n",
      "7b) Answer for tidy up columns names with underscores from ['Ema@il', 'E$$ssense ', 'Pyt[]hon', 'cost_per_head  '] to ['Email', 'Essense', 'Python', 'cost_per_head']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#Fibonacci Sequence\n",
    "def fibonacci_sequence(x):\n",
    "    seq = [0, 1]\n",
    "    if x < 2:\n",
    "        return seq[:x]\n",
    "    else:\n",
    "        for i in range(0, x-len(seq)):\n",
    "            seq.append(seq[-1]+seq[-2])\n",
    "        return seq\n",
    "x = 10\n",
    "print(f'1) Answer for fibonacci sequence with x = {x}: ' + str(fibonacci_sequence(x=x)))\n",
    "print()\n",
    "\n",
    "# divisible list\n",
    "def divisible_list(my_list, y):\n",
    "    div_list = []\n",
    "    for i in my_list:\n",
    "        if i % y == 0:\n",
    "            div_list.append(i)\n",
    "    return div_list\n",
    "my_list = [3, 4, 5, 6, 7, 8, 9]\n",
    "y = 3\n",
    "answer = str(divisible_list(my_list=my_list, y=y))\n",
    "print(f'2) Answer for divisible list with input my_list = {my_list} and y = {y}: {answer}')\n",
    "print()\n",
    "\n",
    "#find lowest common multiple\n",
    "def find_lowest_common_multiple(num1, num2):\n",
    "    larger = num1 if num1 > num2 else num2\n",
    "    while True:\n",
    "        if larger % num1 == 0 and larger % num2 == 0:\n",
    "            return larger\n",
    "        larger += 1\n",
    "#gcd means greatest common divider\n",
    "def find_lcm(num1, num2):\n",
    "    return abs(num1 * num2) // math.gcd(num1, num2)\n",
    "\n",
    "num1 = 9\n",
    "num2 = 63\n",
    "loop_answer = find_lowest_common_multiple(num1, num2)\n",
    "math_answer = find_lcm(num1, num2)\n",
    "\n",
    "print(f'3a) Answer for LCM using loop with inputs num1 = {num1} and num2 = {num2}: {loop_answer}')\n",
    "print(f'3b) Answer for LCM using math package with inputs num1 = {num1} and num2 = {num2}: {math_answer}')\n",
    "print()\n",
    "\n",
    "\n",
    "#compute mean, std and variance\n",
    "def mean_std_variance(my_list):\n",
    "    mean = np.mean(my_list)\n",
    "    std = np.std(my_list)\n",
    "    var = np.var(my_list)\n",
    "    return {'mean': mean, 'std': std, 'variance': var}\n",
    "one_d_array = [1, 5, 3, 1, 2, 4, 6, 7, 1]\n",
    "mean_std_var_answer = mean_std_variance(one_d_array)\n",
    "print(f'4) Answer for mean, std and variance using numpy with inputs my_list = {one_d_array}: {mean_std_var_answer}')\n",
    "print()\n",
    "\n",
    "\n",
    "# bucketing\n",
    "# since I do not see Table_1 has a column call spends, so I will create a new df to perform this task\n",
    "table_1 = pd.DataFrame({'spends': [100000, 500000, 1000000, 2000000]})\n",
    "table_1['Group'] = '3'\n",
    "table_1['Group'] = np.where(table_1['spends'] > 500000, '2', table_1['Group'])\n",
    "table_1['Group'] = np.where(table_1['spends'] > 1000000, '1', table_1['Group'])\n",
    "print(f'5) Answer for bucketing data')\n",
    "print(table_1)\n",
    "print()\n",
    "\n",
    "\n",
    "# date formatting function\n",
    "def format_date(date):\n",
    "    if type(date) == str:\n",
    "        date = datetime.datetime.strptime(date, '%m/%d/%Y')\n",
    "    return date.strftime('%Y%d%m')\n",
    "date = '08/20/2021'\n",
    "date_answer = format_date(date)\n",
    "print(f'6) Answer for formating date from {date} to {date_answer}')\n",
    "print()\n",
    "\n",
    "\n",
    "# cleaning dataframe untidy column names\n",
    "# assume columns have no special characters\n",
    "untidy_df = pd.DataFrame({'Ema@il': [], 'E$$ssense ': [], 'Pyt[]hon': [], 'cost_per_head  ': []})\n",
    "untidy_cols = list(untidy_df.columns)\n",
    "untidy_df.columns = [''.join(e for e in col if e.isalnum()) for col in untidy_df.columns]\n",
    "tidy_cols = list(untidy_df.columns)\n",
    "print(f'7a) Answer for tidy up columns names from {untidy_cols} to {tidy_cols}')\n",
    "print()\n",
    "\n",
    "# if we want to remain underscores in pandas to match db schema\n",
    "untidy_df = pd.DataFrame({'Ema@il': [], 'E$$ssense ': [], 'Pyt[]hon': [], 'cost_per_head  ': []})\n",
    "untidy_cols = list(untidy_df.columns)\n",
    "untidy_df.columns = [re.sub('\\W+', '', col) for col in untidy_df.columns]\n",
    "tidy_cols = list(untidy_df.columns)\n",
    "print(f'7b) Answer for tidy up columns names with underscores from {untidy_cols} to {tidy_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
